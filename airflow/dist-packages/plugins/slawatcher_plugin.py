#!/usr/bin/python2.7

#######################################################################################################################
# slawatcher_plugin.py
# Description: slawatcher to mark dag success when the dag has sensor waiting past the SLA
# Go to the bottom of the code to see what are the operators created in this code
# the code
#######################################################################################################################
import logging
from airflow.models import BaseOperator, DagModel, TaskInstance
from airflow.utils.decorators import apply_defaults
from airflow.plugins_manager import AirflowPlugin
from datetime import datetime
from time import sleep
from airflow.exceptions import AirflowException, AirflowSensorTimeout
from airflow.configuration import conf
from airflow import settings
import airflow.models
from airflow.utils.email import send_email


class SLAWatcher(BaseOperator):
    @apply_defaults
    def __init__(self,
                 delta,
                 kill_all_tasks=False,
                 task_id='sla_watcher',
                 poke_interval=60,
                 timeout=60 * 60 * 24 * 7,
                 send_email=False,
                 to=None,
                 email_subject='',
                 email_html='',
                 *args, **kwargs):
        super(SLAWatcher, self).__init__(task_id=task_id, *args, **kwargs)
        self.delta = delta
        self.kill_all_tasks = kill_all_tasks
        self.poke_interval = poke_interval
        self.timeout = timeout
        self.send_email = send_email
        self.to = to
        self.email_subject = email_subject
        self.email_html = email_html

        if self.send_email:
            if self.to is None or self.to == '':
                raise AirflowException("To email address not provided")
            if self.email_subject is None or self.email_subject == '':
                self.email_subject = 'Automated email generated by ' + context['dag'].dag_id
            if self.email_html is None:
                self.email_html = ''


    def wait_on_success(self, current_task_count, previous_task_count):
        waiting_on_success = False
        if previous_task_count - current_task_count <= 2:
            logging.info("Dag is success")
            waiting_on_success = True
        return waiting_on_success

    def check_if_waiting_on_success(self, context):
        waiting_on_success = False
        # compare counts on previous run and current run to see if the Dag only is waiting for success and the
        # SLAwatcher to finish.  If the difference between today and yesterday is only two, that means, only
        # SLAWatcher(current task) and success task is not completed.
        logging.info('Check if only waiting on success')
        prev_run = context['execution_date'] - context['dag'].schedule_interval
        session = settings.Session()
        TI = TaskInstance
        # counting sensor step in previous run
        previous_task_count = session.query(TI).filter(TI.dag_id == context['dag'].dag_id,
                                                       TI.execution_date == prev_run,
                                                       TI.state == 'success').count()

        current_task_count = session.query(TI).filter(TI.dag_id == context['dag'].dag_id,
                                                      TI.execution_date == context['execution_date'],
                                                      TI.state == 'success').count()

        session.close()

        # if the difference in count between previous run and current run is 2, only two tasks are not complete
        # that is: sla watcher and success task

        waiting_on_success = self.wait_on_success(current_task_count, previous_task_count)

        return waiting_on_success

    def task_success(self,  tis):
        if tis[0].state == 'success':
            logging.info("Dag ran successfully")
            success = True
        else:
            success = False
        return success

    def check_if_success(self, context):
        # check if the whole dag is successful, the last step in the dag is success
        success = False
        session = settings.Session()
        TI = TaskInstance
        tis = session.query(TI).filter(TI.dag_id == context['dag'].dag_id)
        tis = tis.filter(TI.task_id.in_(context['dag'].task_ids))
        tis = tis.filter(TI.task_id == 'success')
        tis = tis.filter(TI.execution_date >= context['execution_date'])
        tis = tis.filter(TI.execution_date <= context['execution_date'])
        if tis.count() == 0:
            logging.info("task id success not found in task instance")
            success = False
        else:
            logging.info("task id success found in task instance")
            success = self.task_success( tis)

        session.close()
        if not success:
            success = self.check_if_waiting_on_success(context)
        return success



    def poke(self, context):
        target_dttm = (
            context['execution_date'] +
            context['dag'].schedule_interval +
            self.delta)
        #logging.info('Checking if the time ({0}) has come'.format(target_dttm))
        return datetime.now() > target_dttm

    def clear_out_instance(self, context):

        outstanding_tasks = True

        while outstanding_tasks:
            # filtering instance down to the same dag and same execution date but not the same task
            session = settings.Session()
            TI = TaskInstance
            tis = session.query(TI)
            tis = tis.filter(TI.dag_id == context['dag'].dag_id)
            tis = tis.filter(TI.task_id.in_(context['dag'].task_ids))
            tis = tis.filter(TI.task_id != context['task'].task_id)
            tis = tis.filter(TI.execution_date >= context['execution_date'])
            tis = tis.filter(TI.execution_date <= context['execution_date'])
            tis = tis.filter(TI.state != 'success')

            # no more outstanding tasks
            if tis.count() == 0:
                logging.info('no outstanding tasks in clear_out_instance')
                outstanding_tasks = False
            else:
                # run this two times, first time is to clear the running task
                # second time is the make sure the shut down or up for retry is cleared
                logging.info('calling airlfow.models.clear_task_instances twice')
                airflow.models.clear_task_instances(tis, session)
                airflow.models.clear_task_instances(tis, session)
                session.commit()

            session.close()

            sleep(5)

            # if there is no outstanding task, just one more check to make sure no task wake up
            if not outstanding_tasks:
                logging.info('Validating that there is no more outstanding tasks')
                session = settings.Session()
                TI = TaskInstance
                tis = session.query(TI)
                tis = tis.filter(TI.dag_id == context['dag'].dag_id)
                tis = tis.filter(TI.task_id.in_(context['dag'].task_ids))
                tis = tis.filter(TI.task_id != context['task'].task_id)
                tis = tis.filter(TI.execution_date >= context['execution_date'])
                tis = tis.filter(TI.execution_date <= context['execution_date'])
                tis = tis.filter(TI.state != 'success')
                if tis.count() > 0:
                    outstanding_tasks = True
                session.close()

    def get_sensor_count(self, context):
        # counting the number of sensor
        logging.info('check if sensor is done')
        prev_run = context['execution_date'] - context['dag'].schedule_interval
        session = settings.Session()
        TI = TaskInstance
        # counting sensor step in previous run
        tis = session.query(TI).filter(TI.dag_id == context['dag'].dag_id,
                                       TI.task_id.in_(context['dag'].task_ids),
                                       (TI.execution_date == prev_run))
        sensor_count = 0

        for ti in tis:
            # make sure only sensor are the tasks that are running and we are still waiting for a trigger

            # if sensor step is running or success
            if 'Sensor' in str(ti.operator) or 'DMStartOperator' in str(ti.operator):
                sensor_count += 1

        session.close()

        logging.info("number of sensor in the dag from previous run =" + str(sensor_count))

        return sensor_count

    def sensor_step(self, context, sensor_count):
        # return false if dag is not running the sensor step yet
        logging.info('check if we are in the sensor step yet, number of sensor in previous run ' + str(sensor_count))
        session = settings.Session()
        TI = TaskInstance
        tis = session.query(TI).filter(TI.dag_id == context['dag'].dag_id)
        tis = tis.filter(TI.task_id.in_(context['dag'].task_ids))
        tis = tis.filter(TI.execution_date >= context['execution_date'])
        tis = tis.filter(TI.execution_date <= context['execution_date'])
        tis = tis.filter(TI.task_id != context['task'].task_id)
        tis = tis.filter(TI.task_id != 'start_batch')
        tis = tis.filter(TI.state.in_(['running', 'success']))

        visible_sensor_count = 0
        completed_sensor_count = 0
        sensor_step = False

        for ti in tis:
            # make sure only sensor are the tasks that are running and we are still waiting for a trigger

            if 'Sensor' in str(ti.operator) or 'DMStartOperator' in str(ti.operator):
                # Counting the number of sesnor that has complete or running
                visible_sensor_count += 1
                # if the sensor
                if ti.state == 'success':
                    completed_sensor_count += 1

        session.close()

        if sensor_count > 0:
            if visible_sensor_count >= sensor_count:
                sensor_step = True
        else:
            # if sensor count is 0, meaning there is no previous run, only check if there has been a sensor step
            if visible_sensor_count > 0:
                sensor_step = True

        return sensor_step

    def check_if_sensor_waiting(self, context, sensor_count):
        # check if only the sensor are not running,assumption is any running tasks that is not sensors
        logging.info('check if sensor waiting')
        session = settings.Session()
        TI = TaskInstance
        tis = session.query(TI).filter(TI.dag_id == context['dag'].dag_id)
        tis = tis.filter(TI.task_id.in_(context['dag'].task_ids))
        tis = tis.filter(TI.execution_date >= context['execution_date'])
        tis = tis.filter(TI.execution_date <= context['execution_date'])
        tis = tis.filter(TI.task_id != context['task'].task_id)
        tis = tis.filter(TI.task_id != 'start_batch')
        tis = tis.filter(TI.state.in_(['running', 'success']))
        sensor_waiting = False
        completed_sensor_count = 0
        for ti in tis:
            logging.info('in loop to check what task is running or success')
            # make sure only sensor are the tasks that are running and we are still waiting for a trigger
            if 'Sensor' in str(ti.operator) or 'DMStartOperator' in str(ti.operator):
                logging.info('sensor name ' + str(ti.operator))
                # Counting the number of sesnor that has complete or running
                # if the sensor
                if ti.state == 'success':
                    logging.info("completed sensor task: " + ti.task_id)
                    completed_sensor_count += 1
                if ti.state == 'running':
                    logging.info('sensor task is running')
                    sensor_waiting = True
        session.close()

        logging.info("completed sensor count: " + str(completed_sensor_count))
        # if all the sensor tasks are completed, that means sensors are not waiting
        if 0 < sensor_count == completed_sensor_count:
            logging.info("the sensors tasks are all completed")
            sensor_waiting = False

        if not sensor_waiting:
            logging.info('Sensors are not waiting')
        else:
            logging.info('Sensors are waiting')

        return sensor_waiting

    def get_success_list(self, context):
        # get the list of success tasks
        logging.info('getting success_list')
        success_list = []
        session = settings.Session()
        TI = TaskInstance
        tis = session.query(TI).filter(TI.dag_id == context['dag'].dag_id)
        tis = tis.filter(TI.task_id.in_(context['dag'].task_ids))
        tis = tis.filter(TI.execution_date >= context['execution_date'])
        tis = tis.filter(TI.execution_date <= context['execution_date'])
        tis = tis.filter(TI.state == 'success')
        success_list.append(ti.task_id for ti in tis)
        session.close()
        return success_list

    def set_is_paused(self, is_paused, dag_id):
        session = settings.Session()
        dm = session.query(DagModel).filter(
            DagModel.dag_id == dag_id).first()
        dm.is_paused = is_paused
        session.commit()

    def execute(self, context):
        started_at = datetime.now()
        # wait until the time condition is met or the dag run to success or the sensor step is running or success

        sensor_count = self.get_sensor_count(context)
        # wait until we have reached or passed the sensor step
        dag_success = self.check_if_success(context)
        if not self.kill_all_tasks:
            while not self.sensor_step(context, sensor_count) and not dag_success:
                logging.info("waiting to get to the sensor step")
                dag_success = self.check_if_success(context)

        target_dttm = (
            context['execution_date'] +
            context['dag'].schedule_interval +
            self.delta)
        logging.info('Checking if the time ({0}) has come'.format(target_dttm))

        while not self.poke(context) and not dag_success:
            sleep(self.poke_interval)
            if (datetime.now() - started_at).seconds > self.timeout:
                raise AirflowSensorTimeout('Snap. Time is OUT.')
            dag_success = self.check_if_success(context)

        logging.info("Time delta met. checking downstream tasks...")

        if not dag_success:
            sensor_waiting = self.check_if_sensor_waiting(context, sensor_count)
            # kill task only if only sensors are waiting, or when the kill_all_task is set to true
            if (not self.kill_all_tasks and sensor_waiting) or self.kill_all_tasks:
                logging.info(
                    "(kill_all_tasks is false and only sensors are waiting) or (the kill_all_task is set to true")

                # pause dag before marking tasks success
                logging.info('pause dag')
                self.set_is_paused(True, context['dag'].dag_id)

                logging.info('clearing out instances')
                self.clear_out_instance(context)

                # if the option to kill all tasks is set, get the list of success task id, since there is no need to
                # mark them success again
                if self.kill_all_tasks:
                    logging.info('killing all tasks')
                else:
                    logging.info('sensors are waiting and SLA is met')

                success_list = self.get_success_list(context)

                for t in context['dag'].tasks:
                    # do not mark this task or any tasks that have completed success
                    if t.task_id != context['task'].task_id and t.task_id not in success_list:
                        t.run(
                            start_date=context['execution_date'],
                            end_date=context['execution_date'],
                            mark_success=True,
                            ignore_dependencies=True)

                logging.info("Done marking all tasks success.")
                # unpause dag after the instances are cleared out
                logging.info('unpause dag')
                self.set_is_paused(False, context['dag'].dag_id)

                if self.send_email:
                    logging.info('send email is True')
                    exec_date = context['execution_date']
                    append_html = '<p>Automated email sent by ' + context['dag'].dag_id + ' for date ' + \
                                  exec_date.strftime('%Y-%m-%dT%H:%M:%S') + '. Please do not reply to email.</p>'
                    if self.email_html is None or self.email_html == '':
                        self.email_html = append_html
                    else:
                        self.email_html += append_html
                    retries = 3
                    cluster = conf.get('core', 'cluster')
                    email_subject = "[airflow-{cluster}] {subject}".format(cluster=cluster,subject=self.subject)
                    for i in range(retries):
                        try:
                            send_email(self.to, email_subject, self.email_html)
                        except:
                            logging.info("Failed to send email")
                            pass
                        break

                else:
                    logging.info('send email is false')
            else:
                logging.info('tasks are started, not killing any tasks')
        else:
            logging.info("Dag ran successfully, exiting SLAWatcher")


# Defining the plugin class
class SLAWatcherPlugin(AirflowPlugin):
    name = "slawatcher_plugin"
    operators = [
        SLAWatcher
    ]
